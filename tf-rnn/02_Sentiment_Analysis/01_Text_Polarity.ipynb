{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "urban-talent",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "confident-patrol",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "sharing-polymer",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import os, re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import wordnet\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "hazardous-rebecca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1,\n",
       "       '@ChargerJenn Thx for answering so quick,I was afraid I was gonna crash twitter with all the spamming I did 2 RR..sorry bout that '],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('data.csv')\n",
    "df.iloc[2].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "random-tennis",
   "metadata": {},
   "source": [
    "## Data Cleaning\n",
    "* Removing punctuations\n",
    "* Numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "settled-teacher",
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = df[\"polarity\"].values\n",
    "text = df['text'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "physical-mining",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['A', 'a', 'aa', ..., 'zythum', 'Zyzomys', 'Zyzzogeton'],\n",
       "      dtype='<U24')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "english_words = np.array([word for word in nltk.corpus.words.words('en')])\n",
    "english_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "starting-marina",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'@ChargerJenn Thx for answering so quick,I was afraid I was gonna crash twitter with all the spamming I did 2 RR..sorry bout that '"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = text[2]\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "tired-carolina",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_sent(sent):\n",
    "    a = re.sub(r'[^a-zA-Z]', ' ', sent)\n",
    "    b = re.sub(r'\\s+', ' ', a).strip().lower()\n",
    "    return b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "polar-linux",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_sents = []\n",
    "for sent in text:\n",
    "    cleaned_sents.append(clean_sent(sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "natural-former",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['kconsidder you never tweet',\n",
       " 'sick today coding from the couch',\n",
       " 'chargerjenn thx for answering so quick i was afraid i was gonna crash twitter with all the spamming i did rr sorry bout that',\n",
       " 'wii fit says i ve lost pounds since last time',\n",
       " 'mrkinetik not a thing i don t really have a life']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_sents[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "prostate-valentine",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500000, 500000)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cleaned_sents), len(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "diagnostic-atlas",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "drawn-essay",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({0: 250275, 1: 249725})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "universal-orientation",
   "metadata": {},
   "source": [
    "### Create ``Word Vectors``"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "compatible-basket",
   "metadata": {},
   "source": [
    "> Find number of unique words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "therapeutic-trunk",
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = Counter()\n",
    "for sent in cleaned_sents:\n",
    "    words = word_tokenize(sent)\n",
    "    for word in words:\n",
    "        counter[word] +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "adjusted-threshold",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "272544"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_words = len(counter)\n",
    "unique_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "surprising-husband",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sized-institute",
   "metadata": {},
   "source": [
    "> Splitting `test` and `train` sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "obvious-field",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(cleaned_sents, targets, test_size=.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "informed-programmer",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bengrossman utoh they moved you up an hour bummer i can t get down there that fast break a leg',\n",
       " 'nite folk i only got hours of sleep last night so i might sleep before jimmyfallon is over maybe']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rational-celebrity",
   "metadata": {},
   "source": [
    "> Create a `word_index` from the `test` data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "decreased-bandwidth",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras_preprocessing.text.Tokenizer at 0x1b71e519a90>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = Tokenizer(num_words=unique_words, oov_token=\"<OOV>\" )\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "handed-stadium",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_indices = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "analyzed-tractor",
   "metadata": {},
   "source": [
    "> Creating a `decord` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "foreign-soviet",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_indices_reversed = dict({(value, key) for (key, value) in word_indices.items()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "entire-input",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decord_text(sent):\n",
    "    return \" \".join([word_indices_reversed[i] for i in sent])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "warming-arthur",
   "metadata": {},
   "source": [
    "> Creating `word_tokens` from the `copra`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "revised-stomach",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tokens = tokenizer.texts_to_sequences(X_train)\n",
    "X_test_tokens = tokenizer.texts_to_sequences(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "subject-fighter",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[75395, 75396, 77, 1383, 9, 32, 97, 378, 1210, 2, 31, 14, 36, 164, 71, 18, 724, 539, 5, 1276]\n"
     ]
    }
   ],
   "source": [
    "print(X_train_tokens[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "quiet-button",
   "metadata": {},
   "source": [
    "> Decording the first `sent`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "pursuant-demand",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bengrossman utoh they moved you up an hour bummer i can t get down there that fast break a leg'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decord_text(X_train_tokens[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "classical-prior",
   "metadata": {},
   "source": [
    "> Padd `suquencing`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "large-network",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_words_in_a_sent = 100\n",
    "X_train_tokens_padded = pad_sequences(X_train_tokens, maxlen=max_words_in_a_sent, truncating='post', padding='post' )\n",
    "X_test_tokens_padded = pad_sequences(X_test_tokens, maxlen=max_words_in_a_sent, truncating='post', padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "formed-modification",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100, 100, 100, 100, 100, "
     ]
    }
   ],
   "source": [
    "for sent in X_train_tokens[:5]:\n",
    "    print(len(sent), end=', ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "corrected-current",
   "metadata": {},
   "source": [
    "> Creating a `Neural` Network.\n",
    "\n",
    "```python\n",
    "\n",
    "        [EmbedingLayer]\n",
    "               |-------\n",
    "            [LSTM]     | Bidirectional (backward = LSTM)\n",
    "               |<------       \n",
    "            [LSTM]     | Bidirectional (backward = GRU)\n",
    "               |-------\n",
    "            [LSTM] \n",
    "               |\n",
    "    [GlobalAveragePooling1D]\n",
    "               |\n",
    "            [Dense]\n",
    "               |\n",
    "            [Dense]\n",
    "               |\n",
    "            [Dense]\n",
    "            /    \\\n",
    "       (polite)  (rude)\n",
    "            \n",
    "            \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "charged-election",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras as keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "christian-lecture",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_10 (Embedding)     (None, 100, 64)           17442816  \n",
      "_________________________________________________________________\n",
      "bidirectional_layer_1 (Bidir (None, 100, 256)          197632    \n",
      "_________________________________________________________________\n",
      "bidirectional_layer_2 (Bidir (None, 100, 128)          144000    \n",
      "_________________________________________________________________\n",
      "lstm_last_layer (LSTM)       (None, 32)                20608     \n",
      "_________________________________________________________________\n",
      "flatten_layer (Flatten)      (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_layer_1 (Dense)        (None, 64)                2112      \n",
      "_________________________________________________________________\n",
      "output_dense_layer (Dense)   (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 17,807,233\n",
      "Trainable params: 17,807,233\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "forward_layer_1 = keras.layers.LSTM(128, return_sequences=True, name=\"forward_layer_1\",\n",
    "                                   dropout=.5\n",
    "                                   )\n",
    "backward_layer_1 = keras.layers.LSTM(128, activation='relu', return_sequences=True, \n",
    "                                     go_backwards=True, name=\"backward_layer_1\",\n",
    "                                    dropout=.5\n",
    "                                    )\n",
    "forward_layer_2 = keras.layers.LSTM(64, return_sequences=True, \n",
    "                                    dropout = .5,\n",
    "                                    name=\"forward_layer_2\",\n",
    "                                   )\n",
    "backward_layer_2 = keras.layers.GRU(64, activation='relu', return_sequences=True, go_backwards=True,\n",
    "                                    dropout = .5,\n",
    "                                    name=\"backward_layer_2\"\n",
    "                                   )\n",
    "\n",
    "model = keras.Sequential([\n",
    "    keras.layers.Embedding(input_dim=unique_words, output_dim=64, input_length=max_words_in_a_sent),\n",
    "    keras.layers.Bidirectional(forward_layer_1,\n",
    "                                    backward_layer = backward_layer_1\n",
    "                                   ,name=\"bidirectional_layer_1\"),\n",
    "    keras.layers.Bidirectional(forward_layer_2,\n",
    "                                backward_layer = backward_layer_2,\n",
    "                                name=\"bidirectional_layer_2\"),\n",
    "    keras.layers.LSTM(32, name=\"lstm_last_layer\"),\n",
    "    keras.layers.Flatten(name=\"flatten_layer\"),\n",
    "    keras.layers.Dense(64, activation='tanh', name=\"dense_layer_1\"),\n",
    "    keras.layers.Dense(1, activation='sigmoid', name=\"output_dense_layer\")\n",
    "], name=\"model_sequential\")\n",
    "model.compile(\n",
    "    loss = keras.losses.binary_crossentropy,\n",
    "    optimizer = keras.optimizers.Adam(),\n",
    "    metrics=[\"acc\"]\n",
    ")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "under-confirmation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(400000,)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "roman-toner",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      " 661/2500 [======>.......................] - ETA: 56:38 - loss: 0.6933 - acc: 0.5003"
     ]
    }
   ],
   "source": [
    "early_stopping = keras.callbacks.EarlyStopping(monitor='val_loss',\n",
    "                                                patience=3,\n",
    "                                                verbose=0\n",
    "                                              )\n",
    "history = model.fit(X_train_tokens_padded, y_train, epochs=10, verbose=1, validation_split=.2, batch_size=128,\n",
    "         callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "worse-writing",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BidirectionaLayer(keras.layers.Layer):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "    def call(self, x):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beginning-reunion",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "individual-stephen",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "graphic-florence",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(keras.Model):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "expanded-discretion",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "outer-amendment",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "personalized-support",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
