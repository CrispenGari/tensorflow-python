{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TensorFlowDataSets-RPS.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0z0WVe5LetML"
      },
      "source": [
        "### Installation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zuk6OHNSeve3"
      },
      "source": [
        "pip install -q tensorflow tensorflow-datasets"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6cikfadSew47"
      },
      "source": [
        "#### Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U9grIZb9eyrT"
      },
      "source": [
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from tensorflow import keras\n",
        "import tensorflow_datasets as tfds"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZxeO5C5pez4W"
      },
      "source": [
        "### Checking datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p_h1KAxde25T",
        "outputId": "dd791cfc-0ddd-4dcb-d72e-15a4478d39e0"
      },
      "source": [
        "print(tfds.list_builders())"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['abstract_reasoning', 'accentdb', 'aeslc', 'aflw2k3d', 'ag_news_subset', 'ai2_arc', 'ai2_arc_with_ir', 'amazon_us_reviews', 'anli', 'arc', 'bair_robot_pushing_small', 'bccd', 'beans', 'big_patent', 'bigearthnet', 'billsum', 'binarized_mnist', 'binary_alpha_digits', 'blimp', 'bool_q', 'c4', 'caltech101', 'caltech_birds2010', 'caltech_birds2011', 'cars196', 'cassava', 'cats_vs_dogs', 'celeb_a', 'celeb_a_hq', 'cfq', 'chexpert', 'cifar10', 'cifar100', 'cifar10_1', 'cifar10_corrupted', 'citrus_leaves', 'cityscapes', 'civil_comments', 'clevr', 'clic', 'clinc_oos', 'cmaterdb', 'cnn_dailymail', 'coco', 'coco_captions', 'coil100', 'colorectal_histology', 'colorectal_histology_large', 'common_voice', 'coqa', 'cos_e', 'cosmos_qa', 'covid19sum', 'crema_d', 'curated_breast_imaging_ddsm', 'cycle_gan', 'deep_weeds', 'definite_pronoun_resolution', 'dementiabank', 'diabetic_retinopathy_detection', 'div2k', 'dmlab', 'downsampled_imagenet', 'dsprites', 'dtd', 'duke_ultrasound', 'emnist', 'eraser_multi_rc', 'esnli', 'eurosat', 'fashion_mnist', 'flic', 'flores', 'food101', 'forest_fires', 'fuss', 'gap', 'geirhos_conflict_stimuli', 'genomics_ood', 'german_credit_numeric', 'gigaword', 'glue', 'goemotions', 'gpt3', 'groove', 'gtzan', 'gtzan_music_speech', 'hellaswag', 'higgs', 'horses_or_humans', 'i_naturalist2017', 'imagenet2012', 'imagenet2012_corrupted', 'imagenet2012_real', 'imagenet2012_subset', 'imagenet_a', 'imagenet_r', 'imagenet_resized', 'imagenet_v2', 'imagenette', 'imagewang', 'imdb_reviews', 'irc_disentanglement', 'iris', 'kitti', 'kmnist', 'lfw', 'librispeech', 'librispeech_lm', 'libritts', 'ljspeech', 'lm1b', 'lost_and_found', 'lsun', 'malaria', 'math_dataset', 'mctaco', 'mnist', 'mnist_corrupted', 'movie_lens', 'movie_rationales', 'movielens', 'moving_mnist', 'multi_news', 'multi_nli', 'multi_nli_mismatch', 'natural_questions', 'natural_questions_open', 'newsroom', 'nsynth', 'nyu_depth_v2', 'omniglot', 'open_images_challenge2019_detection', 'open_images_v4', 'openbookqa', 'opinion_abstracts', 'opinosis', 'opus', 'oxford_flowers102', 'oxford_iiit_pet', 'para_crawl', 'patch_camelyon', 'paws_wiki', 'paws_x_wiki', 'pet_finder', 'pg19', 'places365_small', 'plant_leaves', 'plant_village', 'plantae_k', 'qa4mre', 'qasc', 'quickdraw_bitmap', 'radon', 'reddit', 'reddit_disentanglement', 'reddit_tifu', 'resisc45', 'robonet', 'rock_paper_scissors', 'rock_you', 'salient_span_wikipedia', 'samsum', 'savee', 'scan', 'scene_parse150', 'scicite', 'scientific_papers', 'sentiment140', 'shapes3d', 'smallnorb', 'snli', 'so2sat', 'speech_commands', 'spoken_digit', 'squad', 'stanford_dogs', 'stanford_online_products', 'starcraft_video', 'stl10', 'sun397', 'super_glue', 'svhn_cropped', 'ted_hrlr_translate', 'ted_multi_translate', 'tedlium', 'tf_flowers', 'the300w_lp', 'tiny_shakespeare', 'titanic', 'trec', 'trivia_qa', 'tydi_qa', 'uc_merced', 'ucf101', 'vctk', 'vgg_face2', 'visual_domain_decathlon', 'voc', 'voxceleb', 'voxforge', 'waymo_open_dataset', 'web_questions', 'wider_face', 'wiki40b', 'wikihow', 'wikipedia', 'wikipedia_toxicity_subtypes', 'wine_quality', 'winogrande', 'wmt14_translate', 'wmt15_translate', 'wmt16_translate', 'wmt17_translate', 'wmt18_translate', 'wmt19_translate', 'wmt_t2t_translate', 'wmt_translate', 'wordnet', 'xnli', 'xquad', 'xsum', 'yelp_polarity_reviews', 'yes_no']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NSHwNJ5ie3ea"
      },
      "source": [
        "### Getting data Infomation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YfHKJNp6e7qZ",
        "outputId": "7391cff5-f452-423d-83f0-d5443f965279"
      },
      "source": [
        "builder = tfds.builder('rock_paper_scissors')\n",
        "info = builder.info\n",
        "print(info)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tfds.core.DatasetInfo(\n",
            "    name='rock_paper_scissors',\n",
            "    version=3.0.0,\n",
            "    description='Images of hands playing rock, paper, scissor game.',\n",
            "    homepage='http://laurencemoroney.com/rock-paper-scissors-dataset',\n",
            "    features=FeaturesDict({\n",
            "        'image': Image(shape=(300, 300, 3), dtype=tf.uint8),\n",
            "        'label': ClassLabel(shape=(), dtype=tf.int64, num_classes=3),\n",
            "    }),\n",
            "    total_num_examples=2892,\n",
            "    splits={\n",
            "        'test': 372,\n",
            "        'train': 2520,\n",
            "    },\n",
            "    supervised_keys=('image', 'label'),\n",
            "    citation=\"\"\"@ONLINE {rps,\n",
            "    author = \"Laurence Moroney\",\n",
            "    title = \"Rock, Paper, Scissors Dataset\",\n",
            "    month = \"feb\",\n",
            "    year = \"2019\",\n",
            "    url = \"http://laurencemoroney.com/rock-paper-scissors-dataset\"\n",
            "    }\"\"\",\n",
            "    redistribution_info=,\n",
            ")\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VUxQNMJpe8J0"
      },
      "source": [
        "### Data Preparation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LfI1BqNee_GT"
      },
      "source": [
        "train = tfds.load(name='rock_paper_scissors', split=\"train\")\n",
        "test = tfds.load(name='rock_paper_scissors', split='test')"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oo9cYBiUhgzL"
      },
      "source": [
        "### Iterating over data\n",
        "> To iterate over a tensorflow dataset we do it as follows"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "isR5nRe3gtp3",
        "outputId": "c5d368c2-5806-4101-df63-17eafac7092c"
      },
      "source": [
        "for data in train:\n",
        "  print(data['image'], data['label'])\n",
        "  break"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(\n",
            "[[[254 254 254]\n",
            "  [253 253 253]\n",
            "  [254 254 254]\n",
            "  ...\n",
            "  [251 251 251]\n",
            "  [250 250 250]\n",
            "  [250 250 250]]\n",
            "\n",
            " [[254 254 254]\n",
            "  [254 254 254]\n",
            "  [253 253 253]\n",
            "  ...\n",
            "  [250 250 250]\n",
            "  [251 251 251]\n",
            "  [249 249 249]]\n",
            "\n",
            " [[254 254 254]\n",
            "  [254 254 254]\n",
            "  [254 254 254]\n",
            "  ...\n",
            "  [251 251 251]\n",
            "  [250 250 250]\n",
            "  [252 252 252]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[252 252 252]\n",
            "  [251 251 251]\n",
            "  [252 252 252]\n",
            "  ...\n",
            "  [247 247 247]\n",
            "  [249 249 249]\n",
            "  [248 248 248]]\n",
            "\n",
            " [[253 253 253]\n",
            "  [253 253 253]\n",
            "  [251 251 251]\n",
            "  ...\n",
            "  [248 248 248]\n",
            "  [248 248 248]\n",
            "  [248 248 248]]\n",
            "\n",
            " [[252 252 252]\n",
            "  [253 253 253]\n",
            "  [252 252 252]\n",
            "  ...\n",
            "  [248 248 248]\n",
            "  [247 247 247]\n",
            "  [250 250 250]]], shape=(300, 300, 3), dtype=uint8) tf.Tensor(2, shape=(), dtype=int64)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oO_KP18lh1Rk"
      },
      "source": [
        "### Creating a Numpy data\n",
        "> We are going to scale our data and convert it to a nummpy array"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Sui-N2hhE-8"
      },
      "source": [
        "train_images = np.array([data['image'].numpy()/255 for data in train])\n",
        "train_labels =np.array([data['label'].numpy() for data in train])\n",
        "test_image = np.array([data['image'].numpy()/255   for data in test])\n",
        "test_labels = np.array([data['label'].numpy() for data in test])"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "coydWR5FjMoX",
        "outputId": "0b884de2-31ea-4912-801d-2d576b8b518e"
      },
      "source": [
        "train_images[0]"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[[0.99607843, 0.99607843, 0.99607843],\n",
              "        [0.99215686, 0.99215686, 0.99215686],\n",
              "        [0.99607843, 0.99607843, 0.99607843],\n",
              "        ...,\n",
              "        [0.98431373, 0.98431373, 0.98431373],\n",
              "        [0.98039216, 0.98039216, 0.98039216],\n",
              "        [0.98039216, 0.98039216, 0.98039216]],\n",
              "\n",
              "       [[0.99607843, 0.99607843, 0.99607843],\n",
              "        [0.99607843, 0.99607843, 0.99607843],\n",
              "        [0.99215686, 0.99215686, 0.99215686],\n",
              "        ...,\n",
              "        [0.98039216, 0.98039216, 0.98039216],\n",
              "        [0.98431373, 0.98431373, 0.98431373],\n",
              "        [0.97647059, 0.97647059, 0.97647059]],\n",
              "\n",
              "       [[0.99607843, 0.99607843, 0.99607843],\n",
              "        [0.99607843, 0.99607843, 0.99607843],\n",
              "        [0.99607843, 0.99607843, 0.99607843],\n",
              "        ...,\n",
              "        [0.98431373, 0.98431373, 0.98431373],\n",
              "        [0.98039216, 0.98039216, 0.98039216],\n",
              "        [0.98823529, 0.98823529, 0.98823529]],\n",
              "\n",
              "       ...,\n",
              "\n",
              "       [[0.98823529, 0.98823529, 0.98823529],\n",
              "        [0.98431373, 0.98431373, 0.98431373],\n",
              "        [0.98823529, 0.98823529, 0.98823529],\n",
              "        ...,\n",
              "        [0.96862745, 0.96862745, 0.96862745],\n",
              "        [0.97647059, 0.97647059, 0.97647059],\n",
              "        [0.97254902, 0.97254902, 0.97254902]],\n",
              "\n",
              "       [[0.99215686, 0.99215686, 0.99215686],\n",
              "        [0.99215686, 0.99215686, 0.99215686],\n",
              "        [0.98431373, 0.98431373, 0.98431373],\n",
              "        ...,\n",
              "        [0.97254902, 0.97254902, 0.97254902],\n",
              "        [0.97254902, 0.97254902, 0.97254902],\n",
              "        [0.97254902, 0.97254902, 0.97254902]],\n",
              "\n",
              "       [[0.98823529, 0.98823529, 0.98823529],\n",
              "        [0.99215686, 0.99215686, 0.99215686],\n",
              "        [0.98823529, 0.98823529, 0.98823529],\n",
              "        ...,\n",
              "        [0.97254902, 0.97254902, 0.97254902],\n",
              "        [0.96862745, 0.96862745, 0.96862745],\n",
              "        [0.98039216, 0.98039216, 0.98039216]]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YjUpcC6vjo6_"
      },
      "source": [
        "### Class Names\n",
        "0 - Rock\n",
        "\n",
        "1 - Paper\n",
        "\n",
        "2 - Scissors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wfSaxWgThFF2"
      },
      "source": [
        "class_names = np.array([\"rock\", \"paper\", \"scissor\"])"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cjdKvufNfBi2"
      },
      "source": [
        "### Creating a NN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_S6hEaT-fEkF",
        "outputId": "93913bad-203f-4001-bc60-a452de8d55c1"
      },
      "source": [
        "input_shape = train_images[0].shape\n",
        "input_shape"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(300, 300, 3)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gj5gZ5i2kJFi",
        "outputId": "e940012e-f0ca-45e5-e0f2-399e77e922cb"
      },
      "source": [
        "\n",
        "model = keras.Sequential([\n",
        "    keras.layers.Conv2D(32, (3, 3), input_shape=input_shape, activation='relu'),\n",
        "    keras.layers.MaxPool2D((3,3)) ,\n",
        "    keras.layers.Conv2D(64, (2, 2), activation='relu'),\n",
        "    keras.layers.MaxPool2D((2,2)),\n",
        "    keras.layers.Conv2D(64, (2, 2), activation='relu'),\n",
        "    keras.layers.MaxPool2D((2,2)),\n",
        "    keras.layers.Flatten(),\n",
        "    keras.layers.Dense(64, activation='relu'),\n",
        "    keras.layers.Dense(32, activation='relu'),\n",
        "    keras.layers.Dense(3, activation='softmax')\n",
        "])\n",
        "model.summary()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d (Conv2D)              (None, 298, 298, 32)      896       \n",
            "_________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D) (None, 99, 99, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 98, 98, 64)        8256      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 49, 49, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 48, 48, 64)        16448     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 24, 24, 64)        0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 36864)             0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 64)                2359360   \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 32)                2080      \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 3)                 99        \n",
            "=================================================================\n",
            "Total params: 2,387,139\n",
            "Trainable params: 2,387,139\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LvEJSTvVlXJU"
      },
      "source": [
        "### Combiling the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RYYk9qHalRKQ"
      },
      "source": [
        "model.compile(\n",
        "    optimizer = keras.optimizers.Adam(learning_rate=.0001),\n",
        "    metrics=[\"accuracy\"],\n",
        "    loss = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        ")"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kj-Xb2F6mkO2"
      },
      "source": [
        "### Fitting the ModeL"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G61NVC4hlWAl",
        "outputId": "2f89b1f4-53a7-4957-a2cd-ad82663470ae"
      },
      "source": [
        "EPOCHS = 5\n",
        "BATCH_SIZE = 4\n",
        "VALIDATION_SET = (test_image, test_labels)\n",
        "history = model.fit(train_images, train_labels, epochs=EPOCHS, validation_data=VALIDATION_SET, batch_size=BATCH_SIZE)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "630/630 [==============================] - 41s 13ms/step - loss: 0.7952 - accuracy: 0.6217 - val_loss: 0.3887 - val_accuracy: 0.8360\n",
            "Epoch 2/5\n",
            "630/630 [==============================] - 6s 10ms/step - loss: 0.0271 - accuracy: 0.9946 - val_loss: 0.4615 - val_accuracy: 0.8763\n",
            "Epoch 3/5\n",
            "630/630 [==============================] - 6s 10ms/step - loss: 0.0055 - accuracy: 0.9992 - val_loss: 0.5403 - val_accuracy: 0.8629\n",
            "Epoch 4/5\n",
            "630/630 [==============================] - 6s 9ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.6153 - val_accuracy: 0.8468\n",
            "Epoch 5/5\n",
            "630/630 [==============================] - 6s 10ms/step - loss: 3.5554e-04 - accuracy: 1.0000 - val_loss: 0.6667 - val_accuracy: 0.8468\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zM2IVzrGyRT2"
      },
      "source": [
        "### Model Evaluation Conclusion\n",
        "Our model is performing perfect. The loss on the train_set is almost 0 as well as the validation loss. The accuracy on the train set is `100%` compared to `83%` accuracy on the test set.\n",
        "\n",
        "> The model is just overtraining but giving us good results on the validation set."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yCR7k8Aoy6h7"
      },
      "source": [
        "### Making Predictions\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_zDtH2MHlWEC",
        "outputId": "b74c96b8-98a5-4790-c7f1-ff9c12e159c9"
      },
      "source": [
        "predictions = model.predict(test_image[:10])\n",
        "for i, j in zip(predictions, test_labels[:10]):\n",
        "  print(class_names[np.argmax(i)],\"-------->\", class_names[j])"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "scissor --------> scissor\n",
            "paper --------> paper\n",
            "paper --------> scissor\n",
            "rock --------> rock\n",
            "rock --------> rock\n",
            "rock --------> rock\n",
            "paper --------> paper\n",
            "paper --------> paper\n",
            "scissor --------> scissor\n",
            "paper --------> paper\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b83QOHCV0IVn"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TWq4DlDc0CLs"
      },
      "source": [
        "### Tunning Hyper Parameters -- Keras-Tunner\n",
        "* [Docs](https://www.tensorflow.org/tutorials/keras/keras_tuner)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cl6I5Mt70V82"
      },
      "source": [
        "### Installation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EkJ24NeVlWKP",
        "outputId": "e17aeb9d-24f0-46a8-ef1b-5629d6359796"
      },
      "source": [
        "pip install -q -U keras-tuner"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[?25l\r\u001b[K     |█████▏                          | 10kB 21.1MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 20kB 27.3MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 30kB 31.3MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 40kB 34.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 51kB 34.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▎| 61kB 36.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 71kB 8.8MB/s \n",
            "\u001b[?25h  Building wheel for keras-tuner (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for terminaltables (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pZko7sZ_0oHz"
      },
      "source": [
        "### Importing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5OPoGe010jq4"
      },
      "source": [
        "import kerastuner as kt"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o6H7Kfxi0qTJ"
      },
      "source": [
        "def model_builder(hp):\n",
        "\n",
        "  model = keras.Sequential()\n",
        "  #   we want the model to find the best unit and the activation function for the first layer for us\n",
        "  model.add(keras.layers.Conv2D(hp.Int('units',  min_value=32, max_value=512, step=32),(3, 3), \n",
        "                                input_shape=input_shape, activation=hp.Choice('activation-fn',values=['relu', 'sgd'])))\n",
        "  \n",
        "  model.add(keras.layers.MaxPool2D((3,3)))\n",
        "  model.add(keras.layers.Conv2D(64, (2, 2), activation='relu'))\n",
        "  model.add(keras.layers.MaxPool2D((2,2)))\n",
        "  model.add(keras.layers.Conv2D(64, (2, 2), activation='relu'))\n",
        "  model.add(keras.layers.MaxPool2D((2,2)))\n",
        "  model.add(keras.layers.Flatten())\n",
        "  model.add(keras.layers.Dense(64, activation='relu'))\n",
        "  model.add(keras.layers.Dense(32, activation='relu'))\n",
        "  model.add(keras.layers.Dense(3, activation='softmax'))\n",
        "  model.compile(optimizer=keras.optimizers.Adam(learning_rate=hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])),\n",
        "                loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "                metrics=['accuracy'])\n",
        "  return model"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JA-izSwo2e_a",
        "outputId": "55aa4da7-bf0b-49e8-b0d3-442d3a27f9ce"
      },
      "source": [
        "tuner = kt.Hyperband(model_builder,\n",
        "                     objective='val_accuracy',\n",
        "                     max_epochs=10,\n",
        "                     )"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Reloading Oracle from existing project ./untitled_project/oracle.json\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Reloading Oracle from existing project ./untitled_project/oracle.json\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4RDa3RWm3LzC",
        "outputId": "a3c6deb7-9ae0-4d1b-f9fd-1ee9f3761e9f"
      },
      "source": [
        "tuner.search(train_images, train_labels, validation_data=VALIDATION_SET, epochs=EPOCHS, batch_size=BATCH_SIZE)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Trial 1 Complete [00h 01m 13s]\n",
            "val_accuracy: 0.8951612710952759\n",
            "\n",
            "Best val_accuracy So Far: 0.8951612710952759\n",
            "Total elapsed time: 00h 01m 13s\n",
            "\n",
            "Search: Running Trial #2\n",
            "\n",
            "Hyperparameter    |Value             |Best Value So Far \n",
            "units             |480               |192               \n",
            "activation-fn     |relu              |relu              \n",
            "learning_rate     |0.0001            |0.0001            \n",
            "tuner/epochs      |2                 |2                 \n",
            "tuner/initial_e...|0                 |0                 \n",
            "tuner/bracket     |2                 |2                 \n",
            "tuner/round       |0                 |0                 \n",
            "\n",
            "Epoch 1/2\n",
            "  6/630 [..............................] - ETA: 41s - loss: 1.0874 - accuracy: 0.4917WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0117s vs `on_train_batch_end` time: 0.0462s). Check your callbacks.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0117s vs `on_train_batch_end` time: 0.0462s). Check your callbacks.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "630/630 [==============================] - ETA: 0s - loss: 0.7553 - accuracy: 0.6167"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GlEHL9V55nVZ"
      },
      "source": [
        "> That's basically how the `kerastunner` works"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8X1FkBXi35Bn"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}