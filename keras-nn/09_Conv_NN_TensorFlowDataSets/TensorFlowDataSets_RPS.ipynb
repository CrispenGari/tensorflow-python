{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0z0WVe5LetML"
   },
   "source": [
    "### Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "zuk6OHNSeve3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not install packages due to an OSError: [WinError 5] Access is denied: 'C:\\\\Users\\\\crisp\\\\Documents\\\\anaconda3\\\\Lib\\\\site-packages\\\\~umpy\\\\.libs\\\\libopenblas.PYQHXLVVQ7VESDPUVUADXEVJOBGHJPAY.gfortran-win_amd64.dll'\n",
      "Consider using the `--user` option or check the permissions.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pip install -q tensorflow tensorflow-datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow_datasets\n",
      "  Downloading tensorflow_datasets-4.3.0-py3-none-any.whl (3.9 MB)\n",
      "Collecting promise\n",
      "  Using cached promise-2.3-py3-none-any.whl\n",
      "Requirement already satisfied: six in c:\\users\\crisp\\documents\\anaconda3\\lib\\site-packages (from tensorflow_datasets) (1.15.0)\n",
      "Collecting importlib-resources\n",
      "  Downloading importlib_resources-5.1.3-py3-none-any.whl (26 kB)\n",
      "Requirement already satisfied: attrs>=18.1.0 in c:\\users\\crisp\\documents\\anaconda3\\lib\\site-packages (from tensorflow_datasets) (20.3.0)\n",
      "Requirement already satisfied: future in c:\\users\\crisp\\documents\\anaconda3\\lib\\site-packages (from tensorflow_datasets) (0.18.2)\n",
      "Requirement already satisfied: protobuf>=3.12.2 in c:\\users\\crisp\\documents\\anaconda3\\lib\\site-packages (from tensorflow_datasets) (3.15.3)\n",
      "Collecting tensorflow-metadata\n",
      "  Using cached tensorflow_metadata-0.30.0-py3-none-any.whl (47 kB)\n",
      "Requirement already satisfied: numpy in c:\\users\\crisp\\documents\\anaconda3\\lib\\site-packages (from tensorflow_datasets) (1.19.5)\n",
      "Requirement already satisfied: requests>=2.19.0 in c:\\users\\crisp\\documents\\anaconda3\\lib\\site-packages (from tensorflow_datasets) (2.25.1)\n",
      "Requirement already satisfied: tqdm in c:\\users\\crisp\\documents\\anaconda3\\lib\\site-packages (from tensorflow_datasets) (4.56.0)\n",
      "Requirement already satisfied: termcolor in c:\\users\\crisp\\documents\\anaconda3\\lib\\site-packages (from tensorflow_datasets) (1.1.0)\n",
      "Requirement already satisfied: absl-py in c:\\users\\crisp\\documents\\anaconda3\\lib\\site-packages (from tensorflow_datasets) (0.11.0)\n",
      "Collecting dill\n",
      "  Using cached dill-0.3.3-py2.py3-none-any.whl (81 kB)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\crisp\\documents\\anaconda3\\lib\\site-packages (from requests>=2.19.0->tensorflow_datasets) (1.26.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\crisp\\documents\\anaconda3\\lib\\site-packages (from requests>=2.19.0->tensorflow_datasets) (2020.12.5)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\crisp\\documents\\anaconda3\\lib\\site-packages (from requests>=2.19.0->tensorflow_datasets) (2.10)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in c:\\users\\crisp\\documents\\anaconda3\\lib\\site-packages (from requests>=2.19.0->tensorflow_datasets) (4.0.0)\n",
      "Collecting googleapis-common-protos<2,>=1.52.0\n",
      "  Using cached googleapis_common_protos-1.53.0-py2.py3-none-any.whl (198 kB)\n",
      "Installing collected packages: googleapis-common-protos, tensorflow-metadata, promise, importlib-resources, dill, tensorflow-datasets\n",
      "Successfully installed dill-0.3.3 googleapis-common-protos-1.53.0 importlib-resources-5.1.3 promise-2.3 tensorflow-datasets-4.3.0 tensorflow-metadata-0.30.0\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6cikfadSew47"
   },
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "U9grIZb9eyrT"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZxeO5C5pez4W"
   },
   "source": [
    "### Checking datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "p_h1KAxde25T",
    "outputId": "dd791cfc-0ddd-4dcb-d72e-15a4478d39e0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['abstract_reasoning', 'accentdb', 'aeslc', 'aflw2k3d', 'ag_news_subset', 'ai2_arc', 'ai2_arc_with_ir', 'amazon_us_reviews', 'anli', 'arc', 'bair_robot_pushing_small', 'bccd', 'beans', 'big_patent', 'bigearthnet', 'billsum', 'binarized_mnist', 'binary_alpha_digits', 'blimp', 'bool_q', 'c4', 'caltech101', 'caltech_birds2010', 'caltech_birds2011', 'cars196', 'cassava', 'cats_vs_dogs', 'celeb_a', 'celeb_a_hq', 'cfq', 'cherry_blossoms', 'chexpert', 'cifar10', 'cifar100', 'cifar10_1', 'cifar10_corrupted', 'citrus_leaves', 'cityscapes', 'civil_comments', 'clevr', 'clic', 'clinc_oos', 'cmaterdb', 'cnn_dailymail', 'coco', 'coco_captions', 'coil100', 'colorectal_histology', 'colorectal_histology_large', 'common_voice', 'coqa', 'cos_e', 'cosmos_qa', 'covid19sum', 'crema_d', 'curated_breast_imaging_ddsm', 'cycle_gan', 'd4rl_mujoco_ant', 'd4rl_mujoco_halfcheetah', 'dart', 'davis', 'deep_weeds', 'definite_pronoun_resolution', 'dementiabank', 'diabetic_retinopathy_detection', 'div2k', 'dmlab', 'dolphin_number_word', 'downsampled_imagenet', 'drop', 'dsprites', 'dtd', 'duke_ultrasound', 'e2e_cleaned', 'efron_morris75', 'emnist', 'eraser_multi_rc', 'esnli', 'eurosat', 'fashion_mnist', 'flic', 'flores', 'food101', 'forest_fires', 'fuss', 'gap', 'geirhos_conflict_stimuli', 'gem', 'genomics_ood', 'german_credit_numeric', 'gigaword', 'glue', 'goemotions', 'gpt3', 'gref', 'groove', 'gtzan', 'gtzan_music_speech', 'hellaswag', 'higgs', 'horses_or_humans', 'howell', 'i_naturalist2017', 'imagenet2012', 'imagenet2012_corrupted', 'imagenet2012_real', 'imagenet2012_subset', 'imagenet_a', 'imagenet_r', 'imagenet_resized', 'imagenet_v2', 'imagenette', 'imagewang', 'imdb_reviews', 'irc_disentanglement', 'iris', 'kitti', 'kmnist', 'lambada', 'lfw', 'librispeech', 'librispeech_lm', 'libritts', 'ljspeech', 'lm1b', 'lost_and_found', 'lsun', 'lvis', 'malaria', 'math_dataset', 'mctaco', 'mlqa', 'mnist', 'mnist_corrupted', 'movie_lens', 'movie_rationales', 'movielens', 'moving_mnist', 'multi_news', 'multi_nli', 'multi_nli_mismatch', 'natural_questions', 'natural_questions_open', 'newsroom', 'nsynth', 'nyu_depth_v2', 'ogbg_molpcba', 'omniglot', 'open_images_challenge2019_detection', 'open_images_v4', 'openbookqa', 'opinion_abstracts', 'opinosis', 'opus', 'oxford_flowers102', 'oxford_iiit_pet', 'para_crawl', 'patch_camelyon', 'paws_wiki', 'paws_x_wiki', 'pet_finder', 'pg19', 'piqa', 'places365_small', 'plant_leaves', 'plant_village', 'plantae_k', 'qa4mre', 'qasc', 'quac', 'quickdraw_bitmap', 'race', 'radon', 'reddit', 'reddit_disentanglement', 'reddit_tifu', 'resisc45', 'robonet', 'rock_paper_scissors', 'rock_you', 's3o4d', 'salient_span_wikipedia', 'samsum', 'savee', 'scan', 'scene_parse150', 'schema_guided_dialogue', 'scicite', 'scientific_papers', 'sentiment140', 'shapes3d', 'siscore', 'smallnorb', 'snli', 'so2sat', 'speech_commands', 'spoken_digit', 'squad', 'stanford_dogs', 'stanford_online_products', 'star_cfq', 'starcraft_video', 'stl10', 'story_cloze', 'sun397', 'super_glue', 'svhn_cropped', 'tao', 'ted_hrlr_translate', 'ted_multi_translate', 'tedlium', 'tf_flowers', 'the300w_lp', 'tiny_shakespeare', 'titanic', 'trec', 'trivia_qa', 'tydi_qa', 'uc_merced', 'ucf101', 'vctk', 'vgg_face2', 'visual_domain_decathlon', 'voc', 'voxceleb', 'voxforge', 'waymo_open_dataset', 'web_nlg', 'web_questions', 'wider_face', 'wiki40b', 'wiki_bio', 'wiki_table_questions', 'wiki_table_text', 'wikiann', 'wikihow', 'wikipedia', 'wikipedia_toxicity_subtypes', 'wine_quality', 'winogrande', 'wmt13_translate', 'wmt14_translate', 'wmt15_translate', 'wmt16_translate', 'wmt17_translate', 'wmt18_translate', 'wmt19_translate', 'wmt_t2t_translate', 'wmt_translate', 'wordnet', 'wsc273', 'xnli', 'xquad', 'xsum', 'xtreme_pawsx', 'xtreme_xnli', 'yelp_polarity_reviews', 'yes_no', 'youtube_vis']\n"
     ]
    }
   ],
   "source": [
    "print(tfds.list_builders())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NSHwNJ5ie3ea"
   },
   "source": [
    "### Getting data Infomation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YfHKJNp6e7qZ",
    "outputId": "7391cff5-f452-423d-83f0-d5443f965279"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tfds.core.DatasetInfo(\n",
      "    name='rock_paper_scissors',\n",
      "    full_name='rock_paper_scissors/3.0.0',\n",
      "    description=\"\"\"\n",
      "    Images of hands playing rock, paper, scissor game.\n",
      "    \"\"\",\n",
      "    homepage='http://laurencemoroney.com/rock-paper-scissors-dataset',\n",
      "    data_path='C:\\\\Users\\\\crisp\\\\tensorflow_datasets\\\\rock_paper_scissors\\\\3.0.0',\n",
      "    download_size=Unknown size,\n",
      "    dataset_size=Unknown size,\n",
      "    features=FeaturesDict({\n",
      "        'image': Image(shape=(300, 300, 3), dtype=tf.uint8),\n",
      "        'label': ClassLabel(shape=(), dtype=tf.int64, num_classes=3),\n",
      "    }),\n",
      "    supervised_keys=('image', 'label'),\n",
      "    disable_shuffling=False,\n",
      "    splits={\n",
      "    },\n",
      "    citation=\"\"\"@ONLINE {rps,\n",
      "    author = \"Laurence Moroney\",\n",
      "    title = \"Rock, Paper, Scissors Dataset\",\n",
      "    month = \"feb\",\n",
      "    year = \"2019\",\n",
      "    url = \"http://laurencemoroney.com/rock-paper-scissors-dataset\"\n",
      "    }\"\"\",\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "builder = tfds.builder('rock_paper_scissors')\n",
    "info = builder.info\n",
    "print(info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VUxQNMJpe8J0"
   },
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "LfI1BqNee_GT"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDownloading and preparing dataset Unknown size (download: Unknown size, generated: Unknown size, total: Unknown size) to C:\\Users\\crisp\\tensorflow_datasets\\rock_paper_scissors\\3.0.0...\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0691c967014a4b7fabbd763d8cb7611f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dl Completed...: 0 url [00:00, ? url/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ef4103e8f3149599d9ee3e7536ce9f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dl Size...: 0 MiB [00:00, ? MiB/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating splits...:   0%|          | 0/2 [00:00<?, ? splits/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train examples...: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Shuffling rock_paper_scissors-train.tfrecord...:   0%|          | 0/2520 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test examples...: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Shuffling rock_paper_scissors-test.tfrecord...:   0%|          | 0/372 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDataset rock_paper_scissors downloaded and prepared to C:\\Users\\crisp\\tensorflow_datasets\\rock_paper_scissors\\3.0.0. Subsequent calls will reuse this data.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "train = tfds.load(name='rock_paper_scissors', split=\"train\")\n",
    "test = tfds.load(name='rock_paper_scissors', split='test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oo9cYBiUhgzL"
   },
   "source": [
    "### Iterating over data\n",
    "> To iterate over a tensorflow dataset we do it as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "isR5nRe3gtp3",
    "outputId": "c5d368c2-5806-4101-df63-17eafac7092c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[[254 254 254]\n",
      "  [253 253 253]\n",
      "  [254 254 254]\n",
      "  ...\n",
      "  [251 251 251]\n",
      "  [250 250 250]\n",
      "  [250 250 250]]\n",
      "\n",
      " [[254 254 254]\n",
      "  [254 254 254]\n",
      "  [253 253 253]\n",
      "  ...\n",
      "  [250 250 250]\n",
      "  [251 251 251]\n",
      "  [249 249 249]]\n",
      "\n",
      " [[254 254 254]\n",
      "  [254 254 254]\n",
      "  [254 254 254]\n",
      "  ...\n",
      "  [251 251 251]\n",
      "  [250 250 250]\n",
      "  [252 252 252]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[252 252 252]\n",
      "  [251 251 251]\n",
      "  [252 252 252]\n",
      "  ...\n",
      "  [247 247 247]\n",
      "  [249 249 249]\n",
      "  [248 248 248]]\n",
      "\n",
      " [[253 253 253]\n",
      "  [253 253 253]\n",
      "  [251 251 251]\n",
      "  ...\n",
      "  [248 248 248]\n",
      "  [248 248 248]\n",
      "  [248 248 248]]\n",
      "\n",
      " [[252 252 252]\n",
      "  [253 253 253]\n",
      "  [252 252 252]\n",
      "  ...\n",
      "  [248 248 248]\n",
      "  [247 247 247]\n",
      "  [250 250 250]]], shape=(300, 300, 3), dtype=uint8) tf.Tensor(2, shape=(), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "for data in train:\n",
    "  print(data['image'], data['label'])\n",
    "  break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oO_KP18lh1Rk"
   },
   "source": [
    "### Creating a Numpy data\n",
    "> We are going to scale our data and convert it to a nummpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1Sui-N2hhE-8"
   },
   "outputs": [],
   "source": [
    "train_images = np.array([data['image'].numpy()/255 for data in train])\n",
    "train_labels =np.array([data['label'].numpy() for data in train])\n",
    "test_image = np.array([data['image'].numpy()/255   for data in test])\n",
    "test_labels = np.array([data['label'].numpy() for data in test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "coydWR5FjMoX",
    "outputId": "0b884de2-31ea-4912-801d-2d576b8b518e"
   },
   "outputs": [],
   "source": [
    "train_images[0], train_images.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YjUpcC6vjo6_"
   },
   "source": [
    "### Class Names\n",
    "0 - Rock\n",
    "\n",
    "1 - Paper\n",
    "\n",
    "2 - Scissors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wfSaxWgThFF2"
   },
   "outputs": [],
   "source": [
    "class_names = np.array([\"rock\", \"paper\", \"scissor\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cjdKvufNfBi2"
   },
   "source": [
    "### Creating a NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_S6hEaT-fEkF",
    "outputId": "93913bad-203f-4001-bc60-a452de8d55c1"
   },
   "outputs": [],
   "source": [
    "input_shape = train_images[0].shape\n",
    "input_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Gj5gZ5i2kJFi",
    "outputId": "e940012e-f0ca-45e5-e0f2-399e77e922cb"
   },
   "outputs": [],
   "source": [
    "\n",
    "model = keras.Sequential([\n",
    "    keras.layers.Conv2D(32, (3, 3), input_shape=input_shape, activation='relu'),\n",
    "    keras.layers.MaxPool2D((3,3)) ,\n",
    "    keras.layers.Conv2D(64, (2, 2), activation='relu'),\n",
    "    keras.layers.MaxPool2D((2,2)),\n",
    "    keras.layers.Conv2D(64, (2, 2), activation='relu'),\n",
    "    keras.layers.MaxPool2D((2,2)),\n",
    "    keras.layers.Flatten(),\n",
    "    keras.layers.Dense(64, activation='relu'),\n",
    "    keras.layers.Dense(32, activation='relu'),\n",
    "    keras.layers.Dense(3, activation='softmax')\n",
    "])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LvEJSTvVlXJU"
   },
   "source": [
    "### Combiling the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RYYk9qHalRKQ"
   },
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer = keras.optimizers.Adam(learning_rate=.0001),\n",
    "    metrics=[\"accuracy\"],\n",
    "    loss = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kj-Xb2F6mkO2"
   },
   "source": [
    "### Fitting the ModeL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "G61NVC4hlWAl",
    "outputId": "2f89b1f4-53a7-4957-a2cd-ad82663470ae"
   },
   "outputs": [],
   "source": [
    "EPOCHS = 5\n",
    "BATCH_SIZE = 4\n",
    "VALIDATION_SET = (test_image, test_labels)\n",
    "history = model.fit(train_images, train_labels, epochs=EPOCHS, validation_data=VALIDATION_SET, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zM2IVzrGyRT2"
   },
   "source": [
    "### Model Evaluation Conclusion\n",
    "Our model is performing perfect. The loss on the train_set is almost 0 as well as the validation loss. The accuracy on the train set is `100%` compared to `83%` accuracy on the test set.\n",
    "\n",
    "> The model is just overtraining but giving us good results on the validation set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yCR7k8Aoy6h7"
   },
   "source": [
    "### Making Predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_zDtH2MHlWEC",
    "outputId": "b74c96b8-98a5-4790-c7f1-ff9c12e159c9"
   },
   "outputs": [],
   "source": [
    "predictions = model.predict(test_image[:10])\n",
    "for i, j in zip(predictions, test_labels[:10]):\n",
    "  print(class_names[np.argmax(i)],\"-------->\", class_names[j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b83QOHCV0IVn"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TWq4DlDc0CLs"
   },
   "source": [
    "### Tunning Hyper Parameters -- Keras-Tunner\n",
    "* [Docs](https://www.tensorflow.org/tutorials/keras/keras_tuner)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cl6I5Mt70V82"
   },
   "source": [
    "### Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EkJ24NeVlWKP",
    "outputId": "e17aeb9d-24f0-46a8-ef1b-5629d6359796"
   },
   "outputs": [],
   "source": [
    "pip install -q -U keras-tuner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pZko7sZ_0oHz"
   },
   "source": [
    "### Importing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5OPoGe010jq4"
   },
   "outputs": [],
   "source": [
    "import kerastuner as kt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o6H7Kfxi0qTJ"
   },
   "outputs": [],
   "source": [
    "def model_builder(hp):\n",
    "\n",
    "  model = keras.Sequential()\n",
    "  #   we want the model to find the best unit and the activation function for the first layer for us\n",
    "  model.add(keras.layers.Conv2D(hp.Int('units',  min_value=32, max_value=512, step=32),(3, 3), \n",
    "                                input_shape=input_shape, activation=hp.Choice('activation-fn',values=['relu', 'sgd'])))\n",
    "  \n",
    "  model.add(keras.layers.MaxPool2D((3,3)))\n",
    "  model.add(keras.layers.Conv2D(64, (2, 2), activation='relu'))\n",
    "  model.add(keras.layers.MaxPool2D((2,2)))\n",
    "  model.add(keras.layers.Conv2D(64, (2, 2), activation='relu'))\n",
    "  model.add(keras.layers.MaxPool2D((2,2)))\n",
    "  model.add(keras.layers.Flatten())\n",
    "  model.add(keras.layers.Dense(64, activation='relu'))\n",
    "  model.add(keras.layers.Dense(32, activation='relu'))\n",
    "  model.add(keras.layers.Dense(3, activation='softmax'))\n",
    "  model.compile(optimizer=keras.optimizers.Adam(learning_rate=hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])),\n",
    "                loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                metrics=['accuracy'])\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JA-izSwo2e_a",
    "outputId": "55aa4da7-bf0b-49e8-b0d3-442d3a27f9ce"
   },
   "outputs": [],
   "source": [
    "tuner = kt.Hyperband(model_builder,\n",
    "                     objective='val_accuracy',\n",
    "                     max_epochs=10,\n",
    "                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4RDa3RWm3LzC",
    "outputId": "a3c6deb7-9ae0-4d1b-f9fd-1ee9f3761e9f"
   },
   "outputs": [],
   "source": [
    "tuner.search(train_images, train_labels, validation_data=VALIDATION_SET, epochs=EPOCHS, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GlEHL9V55nVZ"
   },
   "source": [
    "> That's basically how the `kerastunner` works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8X1FkBXi35Bn"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "TensorFlowDataSets-RPS.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
