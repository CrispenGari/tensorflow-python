{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "global-potato",
   "metadata": {},
   "source": [
    "## MNIST dataset\n",
    "* The MNIST database was constructed from NIST's Special Database 3 and Special Database 1 which contain binary images of handwritten digits.\n",
    "> In this example we are going to create a Neural Network that recognizes hand written digits from MNIST dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "handmade-torture",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "moved-trunk",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tensorflow.keras import datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "brilliant-removal",
   "metadata": {},
   "source": [
    "> Checking the `datasets` that we have in keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "coupled-counter",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__path__', '__spec__', '_sys', 'boston_housing', 'cifar10', 'cifar100', 'fashion_mnist', 'imdb', 'mnist', 'reuters']\n"
     ]
    }
   ],
   "source": [
    "print(dir(datasets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "quiet-landscape",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.4.1'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "medieval-atlas",
   "metadata": {},
   "source": [
    "### Data preparation\n",
    "`MNIST` dataset contain images of digits from `0` to `9` which are `28x28`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "elementary-receipt",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alternate-seller",
   "metadata": {},
   "source": [
    "> The `MNIST` object we just created contains 2 tuples, the first one is for train dataset and the otherone is for testing. In each tuple there are 2 thing features and labels. Features is just an `ndarray` of pixels and labels are just numbers from 0 to 9."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "configured-student",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((60000, 28, 28), (60000,), (10000, 28, 28), (10000,))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(X_train, y_train), (X_test, y_test) = mnist\n",
    "X_train.shape, y_train.shape, X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "outstanding-festival",
   "metadata": {},
   "source": [
    "> We have `60000` images which are `28x28` and `60000` labels for each in the train dataset. We also have `10000` images with the same dimension in the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "confirmed-episode",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([7, 2, 1, 0, 4], dtype=uint8)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "successful-defense",
   "metadata": {},
   "source": [
    "> These images seems to be shuffled already. Let's visualise the first image in the train data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "greenhouse-veteran",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(X_train[0], cmap='gray')\n",
    "plt.title(y_train[0], fontsize=14, fontweight=\"bold\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "subsequent-grass",
   "metadata": {},
   "source": [
    "> This is a `5` indeed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pretty-craps",
   "metadata": {},
   "source": [
    "### Data Scaling\n",
    "It's generally a good idea to \"normalize\" your data. This typically involves scaling the data to be between 0 and 1, or maybe -1 and positive 1. In our case, each \"pixel\" is a feature, and each feature currently ranges from 0 to 255. Not quite 0 to 1. Let's change that with a handy utility function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "capable-moldova",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = keras.utils.normalize(X_train, axis=1)\n",
    "X_test = keras.utils.normalize(X_test, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "appropriate-restoration",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(X_train[0], cmap='gray')\n",
    "plt.title(y_train[0], fontsize=14, fontweight=\"bold\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "controlling-intervention",
   "metadata": {},
   "source": [
    "> We still getting the same `5` nothing has changed so far, we just `normalize` pixels to be between 0 and 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ranking-bikini",
   "metadata": {},
   "source": [
    "> Let's check if the data is balanced or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "greenhouse-rating",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 5923,\n",
       " 1: 6742,\n",
       " 2: 5958,\n",
       " 3: 6131,\n",
       " 4: 5842,\n",
       " 5: 5421,\n",
       " 6: 5918,\n",
       " 7: 6265,\n",
       " 8: 5851,\n",
       " 9: 5949}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numbers = {0:0, 1:0, 2:0, 3:0, 4:0, 5:0, 6:0, 7:0, 8:0, 9:0}\n",
    "for i in y_train:\n",
    "    numbers[i] +=1\n",
    "numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "timely-feature",
   "metadata": {},
   "source": [
    "> The data is pretty cool and there's no much of a difference between the frequences of each number."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unique-auction",
   "metadata": {},
   "source": [
    "### Model Creation\n",
    "> This is very important, Our image is a `28x28` which is multi-d array so we need to flatten the dimension of our image. There are many ways we can use `numpy` anything but we are going to use the keras layer `Flatten()` to flatten our image input for us. So this means our first layer in the Nueral Network will be the `Flatten` layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "piano-mathematics",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential([\n",
    "    keras.layers.Flatten(),\n",
    "    keras.layers.Dense(64, activation='relu'),\n",
    "    keras.layers.Dense(64, activation='relu'),\n",
    "    keras.layers.Dense(64, activation='relu'),\n",
    "    keras.layers.Dense(10, activation='softmax'),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "comfortable-matthew",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer= keras.optimizers.Adam(learning_rate=1e-3,),\n",
    "    loss = keras.losses.SparseCategoricalCrossentropy(),\n",
    "    metrics=[\"accuracy\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reasonable-trinity",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "boring-airport",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "7500/7500 [==============================] - 19s 2ms/step - loss: 0.4123 - accuracy: 0.8718\n",
      "Epoch 2/10\n",
      "7500/7500 [==============================] - 15s 2ms/step - loss: 0.1207 - accuracy: 0.9635\n",
      "Epoch 3/10\n",
      "7500/7500 [==============================] - 15s 2ms/step - loss: 0.0850 - accuracy: 0.9734\n",
      "Epoch 4/10\n",
      "7500/7500 [==============================] - 17s 2ms/step - loss: 0.0712 - accuracy: 0.9783\n",
      "Epoch 5/10\n",
      "7500/7500 [==============================] - 16s 2ms/step - loss: 0.0539 - accuracy: 0.9823\n",
      "Epoch 6/10\n",
      "7500/7500 [==============================] - 16s 2ms/step - loss: 0.0473 - accuracy: 0.9858\n",
      "Epoch 7/10\n",
      "7500/7500 [==============================] - 17s 2ms/step - loss: 0.0393 - accuracy: 0.9872\n",
      "Epoch 8/10\n",
      "7500/7500 [==============================] - 20s 3ms/step - loss: 0.0340 - accuracy: 0.9894\n",
      "Epoch 9/10\n",
      "7500/7500 [==============================] - 17s 2ms/step - loss: 0.0327 - accuracy: 0.9890\n",
      "Epoch 10/10\n",
      "7500/7500 [==============================] - 16s 2ms/step - loss: 0.0279 - accuracy: 0.9906\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1c8e434b5b0>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EPOCHS = 10\n",
    "BATCH_SIZE = 8\n",
    "model.fit(X_train, y_train, epochs=EPOCHS, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "confident-comedy",
   "metadata": {},
   "source": [
    "> The `99%` accurate on the train images. and we have a loss of `0.03` which is close to 0. This means our model is cool on the train sample."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "virgin-arrival",
   "metadata": {},
   "source": [
    "### Model Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "unique-background",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten (Flatten)            (8, 784)                  0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (8, 64)                   50240     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (8, 64)                   4160      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (8, 64)                   4160      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (8, 10)                   650       \n",
      "=================================================================\n",
      "Total params: 59,210\n",
      "Trainable params: 59,210\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hungarian-writer",
   "metadata": {},
   "source": [
    "### Evaluating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "french-massage",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 1s 2ms/step - loss: 0.1254 - accuracy: 0.9746\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.12540856003761292, 0.9746000170707703]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "oriental-pittsburgh",
   "metadata": {},
   "source": [
    "> The `97%` accurate on the test images. and we have a loss of `0.13` which is close to 0. This means our model is cool on the test sample. And we are not `overfitting` because the accuracy from the train and test are close to each other."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "divided-dress",
   "metadata": {},
   "source": [
    "### Making Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "creative-messaging",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[8.0563574e-15, 2.3088481e-13, 1.9216269e-09, 6.9906822e-14,\n",
       "        9.9999762e-01, 6.8072867e-14, 6.8319746e-14, 1.1283490e-08,\n",
       "        3.8955368e-09, 2.4145525e-06]], dtype=float32)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = model.predict((X_test[6:7]))\n",
    "predictions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "daily-healthcare",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 1., 0., 0., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.round(predictions) ## The model is saying it's a 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "forty-listing",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, '4')"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(X_test[6], cmap='gray')\n",
    "plt.title(y_test[6], fontsize=20, fontweight='bold')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hydraulic-court",
   "metadata": {},
   "source": [
    "### Saving the Model\n",
    "> To save our model we call the `model.save(__path__)` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "interesting-mauritius",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/hand_written_digits\\assets\n",
      "Model Saved!!\n"
     ]
    }
   ],
   "source": [
    "model.save('models/hand_written_digits')\n",
    "print(\"Model Saved!!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aggregate-associate",
   "metadata": {},
   "source": [
    "### Loading the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "worth-hamburg",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = keras.models.load_model('models/hand_written_digits')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "forbidden-elder",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = clf.predict((X_test[6:7]))\n",
    "np.argmax(predictions) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sorted-pride",
   "metadata": {},
   "source": [
    "> Awesome!! We have trained our model that recognizes images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "associate-graham",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
