{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "labeled-harvey",
   "metadata": {},
   "source": [
    "### Word Embeddings -Keras\n",
    "\n",
    "Problems with One-Hot Encoded Feature Vector Approaches\n",
    "A potential drawback with one-hot encoded feature vector approaches such as N-Grams, bag of words and TF-IDF approach is that the feature vector for each document can be huge. For instance, if you have a half million unique words in your corpus and you want to represent a sentence that contains 10 words, your feature vector will be a half million dimensional one-hot encoded vector where only 10 indexes will have 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "architectural-landing",
   "metadata": {},
   "source": [
    "### Word Embeddings\n",
    "In word embeddings, every word is represented as an n-dimensional dense vector. The words that are similar will have similar vector. Word embeddings techniques such as GloVe and Word2Vec have proven to be extremely efficient for converting words into corresponding dense vectors. The vector size is small and none of the indexes in the vector is actually empty."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "european-montgomery",
   "metadata": {},
   "source": [
    "### Implementation of Word Embedding with Keras\n",
    "> To implement word embeddings, the Keras library contains a layer called ``Embedding()``. The embedding layer is implemented in the form of a class in Keras and is normally used as a first layer in the sequential model for NLP tasks.\n",
    "\n",
    "[Read More](https://stats.stackexchange.com/questions/270546/how-does-keras-embedding-layer-work)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "manufactured-wyoming",
   "metadata": {},
   "source": [
    "> Embedding(200, 32, input_length=50)\n",
    "\n",
    "* The first parameter in the embeddig layer is the size of the vocabulary or the **total number of unique words in a corpus**.\n",
    "* The second parameter is the number of the **dimensions for each word vector**. For instance, if you want each word vector to have 32 dimensions, you will specify 32 as the second parameter. \n",
    "* And finally, the third parameter is the **length of the input sentence**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "genetic-visiting",
   "metadata": {},
   "source": [
    "### Custom Word Embeddings\n",
    "> We are going to create our custom word embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cloudy-lunch",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.text import one_hot\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "outdoor-stage",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "public-drink",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = [\n",
    "    'This is an excellent movie',\n",
    "    'The move was fantastic I like it',\n",
    "    'You should watch it is brilliant',\n",
    "    'Exceptionally good',\n",
    "    'Wonderfully directed and executed I like it',\n",
    "    'Its a fantastic series',\n",
    "    'Never watched such a brillent movie',\n",
    "    'It is a Wonderful movie',\n",
    "    \"horrible acting\",\n",
    "    'waste of money',\n",
    "    'pathetic picture',\n",
    "    'It was very boring',\n",
    "    'I did not like the movie',\n",
    "    'The movie was horrible',\n",
    "    'I will not recommend',\n",
    "    'The acting is pathetic'\n",
    "]\n",
    "sentiments = np.array([1 if i< 8 else 0 for i in range(16)])\n",
    "sentiments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aquatic-found",
   "metadata": {},
   "source": [
    "> The first `8` are positive `reviews` about the move and the last 8 are negative reviews."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "physical-bookmark",
   "metadata": {},
   "source": [
    "> `0` sentiment means a negative review about the movie and `1` is a positive review about the move. as we know the `Embedding()` layer takes `vocabulary` or number of `unique` words. We want to find the total number of `unique` words in the copus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "interstate-cause",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "intensive-banana",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "71"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_words = []\n",
    "for sent in corpus:\n",
    "    words = word_tokenize(sent)\n",
    "    for word in words:\n",
    "        all_words.append(word)\n",
    "len(all_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dangerous-spotlight",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_words = list(set(all_words))\n",
    "len(unique_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "crucial-shaft",
   "metadata": {},
   "source": [
    "> The Embedding layer expects the words to be in **numeric form**. Therefore, we need to convert the sentences in our corpus to numbers. One way to convert text to numbers is by using the ``one_hot`` function from the ``keras.preprocessing.text`` library. The function takes ``sentence`` and the ``total length of the vocabulary and returns the sentence in numeric form``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "continuous-three",
   "metadata": {},
   "outputs": [],
   "source": [
    "voc_len = len(unique_words) + 5 ## we are just adding 5 to unique words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "boring-charger",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[11, 29, 1, 13, 23],\n",
       " [40, 1, 46, 8, 26, 41, 36],\n",
       " [21, 12, 38, 36, 29, 41],\n",
       " [19, 27],\n",
       " [28, 42, 12, 21, 26, 41, 36],\n",
       " [28, 16, 8, 18],\n",
       " [45, 2, 4, 16, 47, 23],\n",
       " [36, 29, 16, 36, 23],\n",
       " [16, 8],\n",
       " [25, 47, 24],\n",
       " [17, 43],\n",
       " [36, 46, 47, 19],\n",
       " [26, 49, 15, 41, 40, 23],\n",
       " [40, 23, 46, 16],\n",
       " [26, 38, 15, 33],\n",
       " [40, 8, 29, 17]]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedded_sentences = [one_hot(sent, voc_len) for sent in corpus]\n",
    "embedded_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "south-consensus",
   "metadata": {},
   "source": [
    "> The embedding layer expects sentences to be of equal size. However, our encoded sentences are of different sizes. One way to make all the sentences of uniform size is to increase the lenght of all the sentences and make it equal to the length of the largest sentence. Let's first find the largest sentence in our corpus and then we will increase the length of all the sentences to the length of the largest sentence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "advanced-exploration",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_count = lambda sentence: len(word_tokenize(sentence))\n",
    "longest_sentence = max(corpus, key=word_count)\n",
    "len_longest_sentence = len(word_tokenize(longest_sentence))\n",
    "len_longest_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "loving-recall",
   "metadata": {},
   "source": [
    "> We want to make all sentences have equal size, so the sentences that has length less than 7 we will fill the gaps of marking them `7` by 0 using `pad_sequences`. The first parameter is the list of **encoded sentences of unequal sizes**, the second parameter is the **size of the longest sentence** or the padding index, while the last parameter is **padding** where you specify post to add padding at the end of sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "satisfactory-gardening",
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_sents = pad_sequences(embedded_sentences, len_longest_sentence, padding=\"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "instructional-peeing",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[11, 29,  1, 13, 23,  0,  0],\n",
       "       [40,  1, 46,  8, 26, 41, 36],\n",
       "       [21, 12, 38, 36, 29, 41,  0],\n",
       "       [19, 27,  0,  0,  0,  0,  0],\n",
       "       [28, 42, 12, 21, 26, 41, 36],\n",
       "       [28, 16,  8, 18,  0,  0,  0],\n",
       "       [45,  2,  4, 16, 47, 23,  0],\n",
       "       [36, 29, 16, 36, 23,  0,  0],\n",
       "       [16,  8,  0,  0,  0,  0,  0],\n",
       "       [25, 47, 24,  0,  0,  0,  0],\n",
       "       [17, 43,  0,  0,  0,  0,  0],\n",
       "       [36, 46, 47, 19,  0,  0,  0],\n",
       "       [26, 49, 15, 41, 40, 23,  0],\n",
       "       [40, 23, 46, 16,  0,  0,  0],\n",
       "       [26, 38, 15, 33,  0,  0,  0],\n",
       "       [40,  8, 29, 17,  0,  0,  0]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_sents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "athletic-bunch",
   "metadata": {},
   "source": [
    "### Creating a Simple Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "driven-friend",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 7, 20)             1000      \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 140)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 16)                2256      \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 3,273\n",
      "Trainable params: 3,273\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = keras.Sequential([\n",
    "    tf.keras.layers.Embedding(voc_len, 20, input_length=len_longest_sentence),\n",
    "    keras.layers.Flatten(),\n",
    "    keras.layers.Dense(16, activation=\"relu\"),\n",
    "    keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "thorough-newton",
   "metadata": {},
   "source": [
    "> A Sequential model and add the ``Embedding`` layer as the first layer to the model. The length of the vocabulary is specified by the ``voc_len`` parameter. The dimension of each word vector will be ``20`` and the ``input_length`` will be the length of the longest sentence, which is ``7``. Next, the ``Embedding`` layer is flattened so that it can be directly used with the densely connected layer. Since it is a ``binary classification`` problem, we use the ``sigmoid`` function as the loss function at the dense layer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "excessive-connecticut",
   "metadata": {},
   "source": [
    "### Compiling the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "useful-adrian",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=keras.optimizers.Adam(lr=1e-3), \n",
    "              loss=keras.losses.BinaryCrossentropy(),\n",
    "              metrics=['accuracy']\n",
    "             )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "heavy-badge",
   "metadata": {},
   "source": [
    "### Trainning the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unexpected-gravity",
   "metadata": {},
   "source": [
    "> First we want to shuffle our datasets and then split them into train and test as usual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "biological-middle",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[11, 29,  1, 13, 23,  0,  0,  1],\n",
       "       [40,  1, 46,  8, 26, 41, 36,  1],\n",
       "       [21, 12, 38, 36, 29, 41,  0,  1],\n",
       "       [19, 27,  0,  0,  0,  0,  0,  1],\n",
       "       [28, 42, 12, 21, 26, 41, 36,  1],\n",
       "       [28, 16,  8, 18,  0,  0,  0,  1],\n",
       "       [45,  2,  4, 16, 47, 23,  0,  1],\n",
       "       [36, 29, 16, 36, 23,  0,  0,  1],\n",
       "       [16,  8,  0,  0,  0,  0,  0,  0],\n",
       "       [25, 47, 24,  0,  0,  0,  0,  0],\n",
       "       [17, 43,  0,  0,  0,  0,  0,  0],\n",
       "       [36, 46, 47, 19,  0,  0,  0,  0],\n",
       "       [26, 49, 15, 41, 40, 23,  0,  0],\n",
       "       [40, 23, 46, 16,  0,  0,  0,  0],\n",
       "       [26, 38, 15, 33,  0,  0,  0,  0],\n",
       "       [40,  8, 29, 17,  0,  0,  0,  0]])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = np.column_stack([padded_sents, sentiments])\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "amber-playlist",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[40, 23, 46, 16,  0,  0,  0,  0],\n",
       "       [16,  8,  0,  0,  0,  0,  0,  0],\n",
       "       [28, 16,  8, 18,  0,  0,  0,  1],\n",
       "       [36, 46, 47, 19,  0,  0,  0,  0],\n",
       "       [45,  2,  4, 16, 47, 23,  0,  1],\n",
       "       [40,  8, 29, 17,  0,  0,  0,  0],\n",
       "       [26, 38, 15, 33,  0,  0,  0,  0],\n",
       "       [19, 27,  0,  0,  0,  0,  0,  1],\n",
       "       [21, 12, 38, 36, 29, 41,  0,  1],\n",
       "       [17, 43,  0,  0,  0,  0,  0,  0],\n",
       "       [11, 29,  1, 13, 23,  0,  0,  1],\n",
       "       [26, 49, 15, 41, 40, 23,  0,  0],\n",
       "       [36, 29, 16, 36, 23,  0,  0,  1],\n",
       "       [28, 42, 12, 21, 26, 41, 36,  1],\n",
       "       [40,  1, 46,  8, 26, 41, 36,  1],\n",
       "       [25, 47, 24,  0,  0,  0,  0,  0]])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.shuffle(data)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "injured-reservation",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train, test = train_test_split(data, random_state=33, test_size= .2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "enclosed-safety",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train[:][:, :7]\n",
    "X_test = test[:][:, :7]\n",
    "\n",
    "y_test = test[:][:,-1]\n",
    "y_train = train[:][:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "placed-parish",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "2/2 - 0s - loss: 0.0722 - accuracy: 1.0000 - val_loss: 0.5488 - val_accuracy: 0.7500\n",
      "Epoch 2/3\n",
      "2/2 - 0s - loss: 0.0693 - accuracy: 1.0000 - val_loss: 0.5456 - val_accuracy: 0.7500\n",
      "Epoch 3/3\n",
      "2/2 - 0s - loss: 0.0669 - accuracy: 1.0000 - val_loss: 0.5420 - val_accuracy: 0.7500\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1cb4c5836a0>"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EPOCHS = 3\n",
    "BATCH_SIZE = 10\n",
    "VALIDATION_SET = (X_test, y_test)\n",
    "model.fit(X_train, y_train, epochs=EPOCHS, batch_size=BATCH_SIZE, verbose=2, validation_data=VALIDATION_SET)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "drawn-limitation",
   "metadata": {},
   "source": [
    "### Making Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "medium-straight",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.]], dtype=float32),\n",
       " array([0, 1, 0, 0]))"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.round(model.predict(X_test[:])), y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "another-facility",
   "metadata": {},
   "source": [
    "> Those are the basics of `custom word embeddings`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dated-october",
   "metadata": {},
   "source": [
    "### Loading Pretrained Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "amended-holmes",
   "metadata": {},
   "source": [
    "[Doccs](https://stackabuse.com/python-for-nlp-word-embeddings-for-deep-learning-in-keras/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "integral-hotel",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
